{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is NLP?</b>\n",
    "\n",
    "Field of study focused on making sense of language using statistics and computers. Basic topics in NLP are Topic Identification & Text Classification. Applications: Chatbots, Sentiment Analysis, Language Translation, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions\n",
    "\n",
    "String with special syntax or structure which allows us to compare the string with another one.\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 4), match='Mark'>\n",
      "<_sre.SRE_Match object; span=(0, 2), match='Hi'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.match('Mark', 'Mark Zukerberg'))\n",
    "\n",
    "word_regex = '\\w+'\n",
    "print(re.match(word_regex, 'Hi there!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Siraj', 'Raval']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Siraj Raval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"Cricket\"\n",
    "sequence = \"Cricket\"\n",
    "\n",
    "if re.match(pattern, sequence):\n",
    "    print (\"Match found!\")\n",
    "else:\n",
    "    print (\"Match not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harsha'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Using Wild Card Characters to search a string pattern\n",
    "re.search(r'Ha..ha', 'Harsha').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdef'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check 'abcdefijkl'\n",
    "\n",
    "# \\w matches single letters/numbers/'_' symbol\n",
    "re.search(r'ab\\wd\\wf', 'abcdefghijkl').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AB@D^F'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\W matches characters except single letters/numbers/ '_' symbol\n",
    "re.search(r'AB\\WD\\WF', 'AB@D^FGHIJKL').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Raja Harsha'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\s matches a single whitespace character like space, newline, tab, return\n",
    "re.search(r'Raja\\sHarsha', 'Raja Harsha').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rajaharsha@gmail.com'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\S matches any character not part of \\s i.e not space, newline, tab, return\n",
    "re.search(r'rajaharsha\\Sgmail.com', 'rajaharsha@gmail.com').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Break string or document into tokens or smaller chunks. It is one step in preparing data for NLP. We can always tokenize using regular expressions with our own rules.\n",
    "\n",
    "Why do we Tokenize?\n",
    "<br>Easier mapping of parts of speech.\n",
    "<br>Matching common words.\n",
    "<br>Removing unwanted tokens.\n",
    "\n",
    "NLTK: Natural Language Tool Kit makes this much simpler.\n",
    "\n",
    "word_tokenize: tokenize a document into words.<br>\n",
    "sent_tokenize: tokenize a document into sentences.<br>\n",
    "regexp_tokenize: tokenize a string or document based on regexp<br>\n",
    "Tweet_tokenize: tokenize a tweet removing the hashtags, mentions and exclamation points.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Raja']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Hi Raja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Naa peru surya.', 'Na illu india.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"Naa peru surya. Na illu india.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re.search & re.match\n",
    "\n",
    "re.search: looks for the pattern in the entire input string.\n",
    "re.match: looks for the pattern only from the starting of the input string.\n",
    "\n",
    "Below examples will illustrate the diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# re.match\n",
    "print (re.match('abc', 'abcdef').group())\n",
    "print (re.match('cde', 'abcdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "cde\n"
     ]
    }
   ],
   "source": [
    "# re.search\n",
    "print (re.search('abc', 'abcdef').group())\n",
    "print (re.search('cde', 'abcdef').group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
